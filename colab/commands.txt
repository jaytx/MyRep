##INSTALL

#commands to run OpenKEonSpark on Google Colaboratory platform
#before running, create a folder in your Google Drive named DBpedia
#uplod in the folder the zip archive containing the dataset given as output from generate.py

#install
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
!sudo apt-get install scala
!wget -q https://archive.apache.org/dist/spark/spark-2.1.1/spark-2.1.1-bin-hadoop2.7.tgz 
!tar xf spark-2.1.1-bin-hadoop2.7.tgz
!python3 -m pip install tensorflowonspark==1.4.4 py4j==0.10.7 pyspark


#connect Google Drive
from google.colab import drive
drive.mount('/content/drive',force_remount=True)


#clone project
!git clone https://github.com/jaytx/OpenKEonSpark.git 
!mv OpenKEonSpark/colab/gpu_info.py /usr/local/lib/python3.6/dist-packages/tensorflowonspark/gpu_info.py
!mv OpenKEonSpark/colab/TFSparkNode.py /usr/local/lib/python3.6/dist-packages/tensorflowonspark/TFSparkNode.py


#unzip dataset
!unzip /content/drive/My\ Drive/DBpedia/1\ samplewidth.zip -d /content/drive/My\ Drive/DBpedia/


##RUN
from google.colab import drive
drive.mount('/content/drive')


#set env vars
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ['SPARK_WORKER_INSTANCES'] = '3'
os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'
os.environ["CUDA_VISIBLE_DEVICES"]="-1"
os.environ["CORES_PER_WORKER"] = "1"
os.environ["MEMORY_PER_WORKER"] = "8g"
os.environ["LIB_CUDA"] = "/usr/local/cuda-10.0/lib64"
os.environ["WORK_DIR_PREFIX"] = "/content/OpenKEonSpark"
os.environ["SPARK_HOME"] = "/content/spark-2.1.1-bin-hadoop2.7"

#execute train-evaluate pipeline
!bash $WORK_DIR_PREFIX/colab/run_sherlock_classification.sh 1 64 "TransH" 0.00001

